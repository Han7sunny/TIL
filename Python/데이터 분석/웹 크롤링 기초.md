# 웹 크롤링 기초
                 
웹 페이지에 있는 정보를 가지고 오는 것
  + 특정 웹사이트에 짧은 시간 동안 과도하게 데이터를 수집하는 행위는 해당 서버에 무리를 주거나 디도스(DDoS) 공격 등으로 감지될 수 있음
  + selenium 라이브러리의 webdriver를 활용하여 웹 브라우저 조작
    + 사이트 접속, 버튼 클릭, 글자 입력 등 웹 브라우저에서 사람이 할 수 있는 일들을 코드를 통해 제어
    + 웹 브라우저에 표시되는 웹 페이지 정보 다운로드
    + webdriver 활용하기 위해 사용 중인 웹 브라우저의 종류에 따라 제어하는 드라이버 필요 : chromedriver ...
      + chromedriver : selenium의 webdriver를 통해 파이썬에서 크롬 브라우저 제어
      + 현재 사용 중인 크롬 버전 확인(크롬 브라우저 우측 상단 [도움말] -> [Chrome정보]를 통해 현재 사용 중인 버전 확인)
      + https://sites.google.com/a/chromium.org/chromedriver/downloads 에서 크롬 버전과 일치하는 크롬 드라이버 파일 다운로드 
  + BeautifulSoup 라이브러리를 활용해 웹 페이지 상의 HTML 데이터에서 필요한 정보 추출
  
### 웹 크롤링 준비
  ```python
  from selenium import webdriver
  from bs4 import BeautifulSoup # pip install beautifulsoup4
  
  driver = webdriver.Chrome('저장한_크롬드라이버_경로') # driver에 명령을 내려 크롬 브라우저 조작
  url = '데이터를_수집할_해당_URL'
  driver.get(url)
  
  html = driver.page_source # 웹 페이지의 HTML 다운로드(문자열 데이터)
 
  soup = BeautifulSoup(html, 'html.parser') # HTML 형식에 맞게 해석하여 원하는 정보 찾을 수 있도록 준비 
  ```
  
### HTML 정보 찾기

##### 크롬 우측 상단 [도구 더보기] -> [개발자 도구] / (F12) / 웹 페이지 내에서 마우스 오른쪽 버튼 클릭 -> [검사], [Elements]에서 웹 페이지의 HTML 구조 확인
  
BeautifulSoup 명령어 사용      

+ select('조건') : HTML 내에 조건을 만족하는 태그 **모두** 선택
  + 태그 속성 활용 
    + 태그명.클래스_명
    + .클래스_명
    + #아이디_값 : 특정 대상을 지정하기 위해 사용되므로 HTML 내에서 한 번만 사용 가능
  + 상위 구조 활용
    + 한 단계 바로 아래 : >
    + 여러 단계(한 단계 포함) 아래 : ""(띄어쓰기) 
  + 태그 그룹에서 하나의 태그 선택
    + 인덱스 번호로 하나의 태그 지정
      ```python
      tags = soup.select('span.name')
      tag_1 = tags[0] # 첫 번째 원소 선택
      ```
  + 선택한 태그에서 정보 가져오기
    + .text : 화면에 보이는 텍스트 부분만 가져오기
    + ['속성명'] : 태그 내 속성값 가져오기
      ```python
      tags = soup.select('a')
      tag_1 = tags[0]
      content = tag_1.text
      link = tag_1['href']
      ```
